<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Yongming Rao's Homepage</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="./source/main.css">
</head>

   <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> 

  <body>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title"></h1>
  </header>

  <article class="post-content">
    <!-- ## About Me -->

<header>
  <img id="pic" style="float: left; width: 140px; margin:0 50px 10px 0" src="./source/rym.jpg" />
  <h1><b>Yongming Rao</b> <br> </h1>
  <br />
   <br />
  <div id="contact">
  <p> PhD student, Department of Automation, Tsinghua University </p>
 <p> Office: Room 624, Main Building</p>
   <p align=left>
	   	<a href="https://raoyongming.github.io/files/CV_YongmingRao.pdf"> CV </a> /
                <a href="mailto:raoyongming95@gmail.com"> Email </a> /
                <a href="https://scholar.google.com/citations?user=3qO6gK4AAAAJ&hl=en"> Google Scholar </a> /
                <a href="https://github.com/raoyongming"> Github </a>
              </p>

  </header>

 <p>I am a first year Ph.D student in the Department of Automation at Tsinghua Univeristy, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. Before that, I received my B.E. in the Department of Electronic Engineering, Tsinghua University. My research interests lie in computer vision, deep learning, reinforcement learning and their applications </p>

<h2 id="news">News</h2>
<ul>
  <li> [2019/02] Two papers are accepted to CVPR 2019. </li>
  <li> [2019/02] I will co-organize the <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> Tutorial on Deep Reinforcement Learning for Computer Vision</a> at CVPR 2019. </li>
</ul>

<h2 id="publications">Publications</h2>

<ul>
  <li>
  	<p><strong>Spherical Fractal Convolution Neural Networks for Point Cloud Recognition</strong><br>
	<strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou<br>
	<em>2019 IEEE Conference on Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2019 <br> </p>
  </li>
  
  <li>
  	<p><strong>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</strong><br>
	Yansong Tang, Dajun Ding, <strong>Yongming Rao</strong>, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou<br>
	<em>2019 IEEE Conference on Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2019 <br>
	<a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a> </p>	
  </li>
	
  <li>
  	<p><strong>Learning Discriminative Aggregation Network for Video-based Face Recognition and Person Re-identification</strong><br>
	<strong>Yongming Rao</strong>,  Jiwen Lu, Jie Zhou<br>
		<em>International Journal of Computer Vision</em> (<strong>IJCV</strong>, <em>IF: 11.54</em>), 2018 <br>
	<a href="https://raoyongming.github.io/files/IJCV_DAN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a>  </p>
  </li>

  <li>
  	<p><strong>Runtime Network Routing for Efficient Image Classification</strong><br>
	<strong>Yongming Rao</strong>,  Jiwen Lu, Ji Lin, Jie Zhou<br>
	<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>T-PAMI</strong>, <em>IF: 9.45</em>), 2018 <br> 
	<a href="https://raoyongming.github.io/files/pami18.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a> </p>
  </li>

  <li>
  	<p><strong>Learning Globally Optimized Object Detector via Policy Gradient</strong><br>
	<strong>Yongming Rao</strong>, Dahua Lin, Jiwen Lu, Jie Zhou<br>
	<em>2018 IEEE Conference on Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), 2018 <strong>spotlight</strong> <br>
	<a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2657.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/cvpr18_supplement.pdf">[Supplement]</a> </p>
  </li>

  <li>
  	<p><strong>Runtime Neural Pruning</strong><br>
	Ji Lin*, <strong>Yongming Rao*</strong> (equal contribution), Jiwen Lu, Jie Zhou<br>
	<em>The Thirty-first Annual Conference on Neural Information Processing Systems</em> (<strong>NIPS</strong>), 2017 <br> 
	<a href="https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a> </p>
  </li>

  <li>
  	<p><strong>Learning Discriminative Aggregation Network for Video-Based Face Recognition</strong><br>
	<strong>Yongming Rao</strong>, Ji Lin, Jiwen Lu, Jie Zhou<br>
	<em>2017 IEEE International Conference on Computer Vision</em> (<strong>ICCV</strong>), 2017 <strong>spotlight</strong> <br> 
	<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Learning_Discriminative_Aggregation_ICCV_2017_paper.pdf">[PDF]</a>  <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a> <a href="https://raoyongming.github.io/files/dan_supplement.pdf">[Supplement]</a>  </p>
  </li>

  <li>
  	<p><strong>Attention-aware Deep Reinforcement Learning for Video Face Recognition</strong><br>
	<strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou<br>
	<em>2017 IEEE International Conference on Computer Vision</em> (<strong>ICCV</strong>), 2017 <br> 
	<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Attention-Aware_Deep_Reinforcement_ICCV_2017_paper.pdf">[PDF]</a>  </p>
  </li>

  <li>
  	<p><strong>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</strong><br>
	Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong><br>
	<em>2017 IEEE 33rd International Conference on Data Engineering</em> (<strong>ICDE</strong>), 2017 <br>
	<a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>  </p>
  </li>

   
</ul>

<h2 id="honners">Honors and Awards</h2>
<ul>
  <li> 2017 Sensetime Undergraduate Scholarship </li>
  <li> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
  <li> 1st place in Momenta Lane Detection Challenge </li>

</ul>

<h2 id="academic">Academic Services</h2>
<ul>
  <li> <strong>Reviewer:</strong> CVPR 2018/2019, ICML 2019, ICCV 2019, NeurIPS 2019, IJCV, ACCV 2018, ICME 2019, ICPR 2018, ICIP 2018/2019, etc. </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <!--
    <h2 class="footer-heading">Chenxi Liu</h2>
    -->

    <div class="footer-col-wrapper">
     <div class="footer-col  footer-col-1">
    	<p class="text"> <a href="mailto:raoyongming95@gmail.com">raoyongming95@gmail.com</a> </p>
      </div>

     <div class="footer-col  footer-col-2">
     	<p class="text"> 
            <a href="https://github.com/raoyongming">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
                </svg>
              </span>

              <span class="username">raoyongming</span>
            </a> </p>

      </div>

      <div class="footer-col  footer-col-3">
          <p class="text">  <a href="https://scholar.google.com/citations?user=3qO6gK4AAAAJ&hl=en"> Google Scholar </a> </p>
      </div>
    </div>

  </div>

</footer>


  


<audio controls="controls" style="display: none;"></audio></body><style type="text/css">#yddContainer{display:block;font-family:Microsoft YaHei;position:relative;width:100%;height:100%;top:-4px;left:-4px;font-size:12px;border:1px solid}#yddTop{display:block;height:22px}#yddTopBorderlr{display:block;position:static;height:17px;padding:2px 28px;line-height:17px;font-size:12px;color:#5079bb;font-weight:bold;border-style:none solid;border-width:1px}#yddTopBorderlr .ydd-sp{position:absolute;top:2px;height:0;overflow:hidden}.ydd-icon{left:5px;width:17px;padding:0px 0px 0px 0px;padding-top:17px;background-position:-16px -44px}.ydd-close{right:5px;width:16px;padding-top:16px;background-position:left -44px}#yddKeyTitle{float:left;text-decoration:none}#yddMiddle{display:block;margin-bottom:10px}.ydd-tabs{display:block;margin:5px 0;padding:0 5px;height:18px;border-bottom:1px solid}.ydd-tab{display:block;float:left;height:18px;margin:0 5px -1px 0;padding:0 4px;line-height:18px;border:1px solid;border-bottom:none}.ydd-trans-container{display:block;line-height:160%}.ydd-trans-container a{text-decoration:none;}#yddBottom{position:absolute;bottom:0;left:0;width:100%;height:22px;line-height:22px;overflow:hidden;background-position:left -22px}.ydd-padding010{padding:0 10px}#yddWrapper{color:#252525;z-index:10001;background:url(chrome-extension://eopjamdnofihpioajgfdikhhbobonhbb/ab20.png);}#yddContainer{background:#fff;border-color:#4b7598}#yddTopBorderlr{border-color:#f0f8fc}#yddWrapper .ydd-sp{background-image:url(chrome-extension://eopjamdnofihpioajgfdikhhbobonhbb/ydd-sprite.png)}#yddWrapper a,#yddWrapper a:hover,#yddWrapper a:visited{color:#50799b}#yddWrapper .ydd-tabs{color:#959595}.ydd-tabs,.ydd-tab{background:#fff;border-color:#d5e7f3}#yddBottom{color:#363636}#yddWrapper{min-width:250px;max-width:400px;}</style></html>
