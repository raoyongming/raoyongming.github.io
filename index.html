<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yongming Rao</title>
  
  <meta name="author" content="Yongming Rao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yongming Rao</name>
              </p>
              <p> 
                I am a second year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2018, I obtained my B.Eng. in the Department of Electronic Engineering, Tsinghua University.
              </p>
              <p>
              I am broadly interested in computer vision and deep learning. My current research focuses on data-efficient deep learning methods, 3D vision and visual reasoning.
              </p>
              <p style="text-align:center">
                <a href="mailto:raoyongming95@gmail.com">Email</a> &nbsp/&nbsp
                <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=3qO6gK4AAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/raoyongming"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/rym.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2020-07:</b> Two papers on network self knowledge distillation and video-based person re-identification are accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-06:</b> Our team (I and <a href="http://ivg.au.tsinghua.edu.cn/people/Guangyi_Chen/">Guangyi</a>) won the 2nd place in Semi-Supervised Recognition Challenge at <a href="https://sites.google.com/view/fgvc7">FGVC7</a> (CVPR 2020).
              </li>
              <li style="margin: 5px;" >
                <b>2020-02:</b> Three papers on unsupervised 3D understanding and image/face super-resoluation are accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-06:</b> I co-organized the <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> Tutorial on Deep Reinforcement Learning for Computer Vision</a> at <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-02:</b> Two papers on robust point cloud analysis and instructional video understanding are accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PointGLR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.12971">[arXiv]</a> <a href="https://github.com/raoyongming/PointGLR">[Code]</a>
              <br>
              <p></p>
              <p>We present an unsupervised point cloud representation learning method based on global-local bidirectional reasoning, which largely advances the state-of-the-art of unsupervised point cloud understanding and  <strong>outperforms recent supervised methods</strong>.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SPSR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structure-Preserving Super Resolution with Gradient Guidance</papertitle>
              <br>
              Cheng Ma, <strong>Yongming Rao</strong>, Yean Cheng, Ce Chen, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.13081">[arXiv]</a> <a href="https://github.com/Maclory/SPSR">[Code]</a> 
              <br>
              <p> We propose to leverage gradient information as an extra supervision signal to restore structures while generating natural SR images. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/faceSR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Face Super-Resolution with Iterative Collaboration between Attentive Recovery and Landmark Estimation</papertitle>
              <br>
              Cheng Ma,  Zhenyu Jiang, <strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.13063">[arXiv]</a> <a href="https://github.com/Maclory/Deep-Iterative-Collaboration">[Code]</a> 
              <br>
              <p> We propose a deep face super-resolution method with iterative collaboration between two recurrent networks which focus on facial image recovery and landmark estimation respectively </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SFCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Spherical Fractal Convolution Neural Networks for Point Cloud Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/SFCNN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/SFCNN_supplement.pdf">[Supplement]</a> 
              <br>
              <p> We designed Spherical Fractal Convolution Neural Networks (SFCNN) for rotation-invariant point cloud feature learning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/COIN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
              <br>
              Yansong Tang, Dajun Ding, <strong>Yongming Rao</strong>, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a>  <a href="https://github.com/coin-dataset/annotation-tool">[Annotation Tool]</a> 
              <br>
              <p> COIN is the largest and most comprehensive instructional video analysis dataset with rich annotations. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Network Routing for Efficient Image Classification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  Jiwen Lu, Ji Lin, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI, IF: 17.73)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/pami18.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a>
              <br>
              <p> We propose a generic Runtime Network Routing (RNR) framework for efficient image classification, which selects an optimal path inside the network. Our method can be applied to off-the-shelf neural network structures and
                easily extended to various application scenarios. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%"  src='images/IJCV_DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-based Face Recognition and Person Re-identification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  Jiwen Lu, Jie Zhou
              <br>
              <em>International Journal of Computer Vision (IJCV, IF: 6.07)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/IJCV_DAN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video-based face recognition and person re-identification, which aims to integrate
                information from video frames for feature representation effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GODet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Globally Optimized Object Detector via Policy Gradient</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Dahua Lin, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2657.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/cvpr18_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a simple yet effective method to learn globally optimized detector for object detection by directly optimizing mAP using the REINFORCE algorithm. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Neural Pruning</papertitle>
              <br>
              Ji Lin*, <strong>Yongming Rao*</strong> (equal contribution), Jiwen Lu, Jie Zhou
              <br>
              <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2017
              <br>
              <a href="https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a>
              <br>
              <p> We propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-Based Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Ji Lin, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2017
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Learning_Discriminative_Aggregation_ICCV_2017_paper.pdf">[PDF]</a>  <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a> <a href="https://raoyongming.github.io/files/dan_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video face recognition, which aims to integrate information from video frames effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ADRL.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Attention-aware Deep Reinforcement Learning for Video Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Attention-Aware_Deep_Reinforcement_ICCV_2017_paper.pdf">[PDF]</a>
              <br>
              <p> We propose an attention-aware deep reinforcement learning (ADRL) method for video face recognition,
                which aims to discard the misleading and confounding frames and find the focuses of attentions in face videos for person recognition. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (ICDE)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewers Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2018/2019/2020, ICML 2019/2020, ICCV 2019, NeurIPS 2019/2020, ECCV 2020, AAAI 2020, WACV 2020/2021, ACCV 2018/2020, ICME 2019/2020, ICPR 2018/2020, ICIP 2018/2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  IJCV, T-IP, T-MM, T-Cybernetics, IEEE Access
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
